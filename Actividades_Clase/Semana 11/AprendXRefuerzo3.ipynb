{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1cKexFe6kd7G7C6WD0DddrTEKz6mBQDLa","timestamp":1724328989774},{"file_id":"11rjkadEZhK2W0r4iUu4A9c8hzcdst46i","timestamp":1724245926532}],"authorship_tag":"ABX9TyPnulfCHbx3wR2rh5yFsBUi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# 1. Introducción a los principales algoritmos de RL:\n","# Define el entorno del juego\n","class Environment:\n","  def __init__(self):\n","    self.state_space = [0, 1, 2, 3]\n","    self.action_space = [0, 1]\n","    self.rewards = {0: -1, 1: -1, 2: -1, 3: -10}\n","\n","# Crea una instancia del entorno\n","env = Environment()\n","\n","# Muestra información del entorno\n","print(\"Estados posibles: \", env.state_space)\n","print(\"Acciones posibles: \", env.action_space)\n","print(\"Recompensas: \", env.rewards)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PM4aGqe1I161","executionInfo":{"status":"ok","timestamp":1724370621023,"user_tz":300,"elapsed":229,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"9dd2d045-94e7-474d-b8be-f205f7e11e0f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Estados posibles:  [0, 1, 2, 3]\n","Acciones posibles:  [0, 1]\n","Recompensas:  {0: -1, 1: -1, 2: -1, 3: -10}\n"]}]},{"cell_type":"code","source":["# 2. Q-Learning:\n","import numpy as np\n","\n","# Inicializa la tabla Q con valores arbitriarios\n","Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","# Define los parametros del algoritmo\n","alpha = 0.1\n","gamma = 0.9\n","epsilon = 0.1\n","# Entrena el agente utilizando Q-learning\n","for _ in range(1000):\n","  state = np.random.choice(env.state_space)\n","  while state != 3:\n","    action = np.random.choice(env.action_space)\n","    next_state = state + action\n","    reward = env.rewards[next_state]\n","    Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n","    state = next_state\n","\n","# Muestra la función Q-valor aprendida\n","print(\"Función Q-valor aprendida: \")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cyzx9FyMFUJW","outputId":"9cc40bef-3021-4d65-8630-3434d99071c5","executionInfo":{"status":"ok","timestamp":1724370644871,"user_tz":300,"elapsed":220,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Función Q-valor aprendida: \n","[[ -8.73860796  -9.96879051]\n"," [ -9.97085492  -9.99334293]\n"," [ -9.99386377 -10.        ]\n"," [  0.           0.        ]]\n"]}]},{"cell_type":"code","source":["# 3. Sarsa\n","# Reinicia la tabla Q con valores arbitriarios\n","Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","# Entrena el agente usando Sarsa\n","for _ in range(1000):\n","  state = np.random.choice(env.state_space)\n","  action = np.random.choice(env.action_space)\n","  while state != 3:\n","    next_state = state + action\n","    next_action = np.random.choice(env.action_space)\n","    reward = env.rewards[next_state]\n","    Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n","    state = next_state\n","    action = next_action\n","\n","# Muestra la función Q-valor aprendida\n","print(\"Función Q-valor aprendida: \")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLM2pQYAQP8g","executionInfo":{"status":"ok","timestamp":1724370652847,"user_tz":300,"elapsed":241,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"76077e53-194b-4279-815b-532657166593"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Función Q-valor aprendida: \n","[[ -9.99995077  -9.99999999]\n"," [-10.         -10.        ]\n"," [-10.         -10.        ]\n"," [  0.           0.        ]]\n"]}]},{"cell_type":"code","source":["# 4. Política de Gradiente de Montecarlo\n","import numpy as np\n","# Crea una instancia del entorno\n","env = Environment()\n","# Inicia la política con probabilidades uniformes\n","policy = np.ones((len(env.state_space), len(env.action_space))) / len(env.action_space)\n","\n","max_steps = 1000  # Define el límite máximo de pasos\n","steps = 0\n","\n","# Define la función de recompensa promedio\n","def average_reward(Q):\n","  return np.mean([Q[state, np.argmax(policy[state])] for state in env.state_space])\n","\n","# Entrena la política utilizando Gradiente de Montecarlo\n","for _ in range(1000):\n","  state = np.random.choice(env.state_space)\n","  while state != 3 and steps < max_steps:\n","    action = np.random.choice(env.action_space, p=policy[state])\n","    next_state = state + action\n","    reward = env.rewards[next_state]\n","    gradient = np.zeros_like(policy[state])\n","    gradient[action] = 1\n","    alpha = 0.01\n","    policy[state] += alpha * gradient * (reward - average_reward(Q))\n","    steps += 1\n","    # Normalize probabilities to sum to 1\n","    policy[state] /= np.sum(policy[state]) # Add this line to normalize probabilities\n","    state = next_state\n","\n","# Muestra la política aprendida\n","print(\"Política aprendida: \")\n","print(policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQIVSL2BRfIt","executionInfo":{"status":"ok","timestamp":1724372516389,"user_tz":300,"elapsed":199,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"22de3f78-249c-4bc9-ff29-27ad9ddbf5f3"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Política aprendida: \n","[[4.17391207e-01 5.82608793e-01]\n"," [6.66124302e-01 3.33875698e-01]\n"," [1.00000000e+00 1.74474133e-27]\n"," [5.00000000e-01 5.00000000e-01]]\n"]}]}]}