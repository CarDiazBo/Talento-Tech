{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11rjkadEZhK2W0r4iUu4A9c8hzcdst46i","timestamp":1724245926532}],"authorship_tag":"ABX9TyNOnJNQ97sFPnncRh2QZQu5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Proceso de Decisión de Markov (MDP)\n","import numpy as np\n","import random\n","\n","# Defincion de estados, acciones y recompensas\n","estados = ['A', 'B', 'C']\n","acciones = ['Arriba', 'Abajo']\n","recompensas = np.random.randint(0, 10, size=(len(estados), len(acciones)))\n","\n","# Función de transición aleatoria\n","def transicion_aleatoria():\n","    return np.random.choice(estados)\n","\n","# Generación de datos\n","estado_actual = np.random.choice(estados)\n","accion = np.random.choice(acciones)\n","nuevo_estado = transicion_aleatoria()\n","recompensa = recompensas[estados.index(estado_actual), acciones.index(accion)]\n","\n","# Imprimir resultados\n","print(f\"Estado actual: {estado_actual}\")\n","print(f\"Acción: {accion}\")\n","print(f\"Nuevo estado: {nuevo_estado}\")\n","print(f\"Recompensa: {recompensa}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PM4aGqe1I161","executionInfo":{"status":"ok","timestamp":1724250295128,"user_tz":300,"elapsed":231,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"26c8f601-d1c2-49cf-d0bc-61431de68b09"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Estado actual: C\n","Acción: Arriba\n","Nuevo estado: A\n","Recompensa: 7\n"]}]},{"cell_type":"code","source":["# Conceptos de MDP\n","def calcular_valor_estado(mdp, gamma=0.9, theta=0.01):\n","  valores = {estado: 0 for estado in mdp.estados}\n","  while True:\n","    delta = 0\n","    for estado in mdp.estados:\n","      valor_previo = valores[estado]\n","      valores[estado] = sum(mdp.transiciones[estado][accion][nuevo_estado] * (mdp.recompensas[estado][accion][nuevo_estado] + gamma * valores[nuevo_estado]) for accion, in mdp.acciones for nuevo_estado in mdp.estados)\n","      delta = max(delta, abs(valor_previo - valores[estado]))\n","    if delta < theta:\n","      break\n","  return valores\n","\n","  # Ejemplo de uso\n","  valores_estados = calcular_valor_estado(mdp)\n","  print(\"Valores de los estados: \", valores_estados)"],"metadata":{"id":"geuh5iSnM0UN","executionInfo":{"status":"ok","timestamp":1724250298727,"user_tz":300,"elapsed":176,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Propiedades de Markov\n","class MDP:\n","    def __init__(self, estados, acciones, transiciones, recompensas):\n","        self.estados = estados\n","        self.acciones = acciones\n","        self.transiciones = transiciones\n","        self.recompensas = recompensas\n","\n","\n","def verificar_propiedad_markov(mdp):\n","  for estado in mdp.estados:\n","    for accion in mdp.acciones:\n","      suma_probabilidades = sum(mdp.transiciones[estado][accion].values())\n","      if not np.isclose(suma_probabilidades, 1):\n","        return False\n","  return True\n","\n","  # ejemplo de uso\n","  print (verificar_propiedad_markov(mdp))\n","\n","transiciones = {\n","    'A': {'Arriba': {'A': 0.3, 'B': 0.7}, 'Abajo': {'C': 1.0}},\n","    'B': {'Arriba': {'A': 0.6, 'B': 0.4}, 'Abajo': {'C': 1.0}},\n","    'C': {'Arriba': {'A': 1.0}, 'Abajo': {'C': 1.0}}\n","}\n","recompensas = {\n","    'A': {'Arriba': {'A': 1, 'B': 2}, 'Abajo': {'C': 3}},\n","    'B': {'Arriba': {'A': 4, 'B': 5}, 'Abajo': {'C': 6}},\n","    'C': {'Arriba': {'A': 7}, 'Abajo': {'C': 8}}\n","}\n","\n","# Crea un objeto MDP\n","mdp = MDP(['A', 'B', 'C'], ['Arriba', 'Abajo'], transiciones, recompensas)\n","\n","if verificar_propiedad_markov(mdp):\n","  print(\"El MDP cumple con la propiedad de Markov.\")\n","else:\n","  print(\"El MDP no cumple con la propiedad de Markov.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7BV4KNgSgTc","executionInfo":{"status":"ok","timestamp":1724250303720,"user_tz":300,"elapsed":262,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"efe90925-522a-43cc-90d4-9f016dea4b78"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["El MDP cumple con la propiedad de Markov.\n"]}]},{"cell_type":"code","source":["# Propiedad de Recompensa\n","def calcular_recompensa_promedio(mdp):\n","  recompensa_total = 0\n","  total_transiciones = 0\n","  for estado in mdp.estados:\n","    for accion in mdp.acciones:\n","      for nuevo_estado in mdp.transiciones[estado][accion]:\n","        recompensa_total += mdp.transiciones[estado][accion][nuevo_estado] * mdp.recompensas[estado][accion][nuevo_estado]\n","        total_transiciones += mdp.transiciones[estado][accion][nuevo_estado]\n","  return recompensa_total / total_transiciones\n","\n","  # ejemplo de uso\n","  print (\"Recompensa promedio por acción: \", calcular_recompensa_promedio(mdp))"],"metadata":{"id":"C8YSgwHVShfQ","executionInfo":{"status":"ok","timestamp":1724250463506,"user_tz":300,"elapsed":247,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# recompensa promedio por acción\n","def calcular_recompensa_promedio(mdp):\n","  recompensa_total = 0\n","  total_transiciones = 0\n","  for estado in mdp.estados:\n","    for accion in mdp.acciones:\n","      for nuevo_estado in mdp.transiciones[estado][accion]:\n","        recompensa_total += mdp.transiciones[estado][accion][nuevo_estado] * mdp.recompensas[estado][accion][nuevo_estado]\n","        total_transiciones += mdp.transiciones[estado][accion][nuevo_estado]\n","  return recompensa_total / total_transiciones"],"metadata":{"id":"206SIqYqVnPU"},"execution_count":null,"outputs":[]}]}