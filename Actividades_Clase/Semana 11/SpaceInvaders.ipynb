{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5/aXON+KnOMiF16YCgQxf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# https://github.com/RicardoMoya/Reinforcemente_Learning_with_Python/blob/master/4_02_RL_SpaceInvaders_GYM_KerasRL.ipynb\n","from IPython.core.display import display, HTML\n","display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n","\n","# Filtramos los warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Forzar uso CPU en caso de dispones GPU en el PC\n","import os\n","os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"BAo3seiYL85z","executionInfo":{"status":"ok","timestamp":1724295428889,"user_tz":300,"elapsed":389,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"cf34dca0-f407-431e-8439-501103199869"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>.container { width:80% !important; }</style>"]},"metadata":{}}]},{"cell_type":"markdown","source":["Juego Space Invaders de Open AI GYM (con Keras RL)\n","GYM de Open AI https://gym.openai.com/: Gym es un conjunto de herramientas para desarrollar y comparar algoritmos de Aprendizaje por Refuerzo (RL). Esta librería ofrece diferentes juegos (entornos) como los juegos de Atari en los que poder probar los algoritmos de RL.\n","\n","Keras RL https://keras-rl.readthedocs.io/: keras-rl implementa algunos algoritmos de Aprendizaje por Refuerzo en Python y se integra con con Tensorflow-Keras. Además, keras-rl esta listo para su uso con OpenAI Gym, por lo que resulta sencillo evaluar mediante los juegos de GYM el rendimiento de los Algoritmos."],"metadata":{"id":"z0K4Zs0QD3ZK"}},{"cell_type":"code","source":["!pip install tensorflow==2.10.0\n","!pip install keras-rl2\n","!pip3 install atari_py --user\n","!pip install gym[atari,accept-rom-license]==0.23.0  # Reinstall with Atari support"],"metadata":{"id":"la_Q7WTpMAh4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1.- Instalación de las librerías\n","!pip install tensorflow==2.10.0\n","#!pip uninstall atari_py -y  # Uninstall any existing versions\n","!pip install atari_py  # Reinstall without --user\n","!pip install gym[atari]  # Reinstall gym with Atari dependencies\n","\n","# 2.- Importamos la librería de GYM y atari\n","import gym\n","import atari_py  # Try importing again\n","\n","# Definición de constantes\n","ENVIRONMENT_NAME = \"SpaceInvaders-v0\"\n","\n","# 3.- Instanciamos el entorno del Space Invaders y jugamos con un agente que toma acciones aleatorias\n","# Instanciamos el Entorno\n","env = gym.make(ENVIRONMENT_NAME)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JkfQCIrnM4US","executionInfo":{"status":"ok","timestamp":1724295742495,"user_tz":300,"elapsed":8962,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"bc73c752-d63b-48b6-f62b-f28131959180"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: atari-py 0.2.9\n","Uninstalling atari-py-0.2.9:\n","  Successfully uninstalled atari-py-0.2.9\n","Collecting atari_py\n","  Using cached atari_py-0.2.9-cp310-cp310-linux_x86_64.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari_py) (1.26.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from atari_py) (1.16.0)\n","Installing collected packages: atari_py\n","Successfully installed atari_py-0.2.9\n","Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.23.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n","Requirement already satisfied: ale-py~=0.7.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.7.5)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.4->gym[atari]) (6.4.2)\n"]}]},{"cell_type":"code","source":["# Obtenemos el tamaño de las imagenes del juego y los canales\n","height, width, n_channels = env.observation_space.shape\n","print('Tamaño de la imagen: {}x{} - Nº Canales: {}'.format(width, height, n_channels))\n","\n","# Vemos las posibles acciones del juego:\n","n_actions = env.action_space.n\n","actions_info = env.unwrapped.get_action_meanings()\n","print('\\nNº de acciones: {} - Acciones: {}\\n'.format(n_actions, actions_info))\n","\n","# Jugamos 2 partidas de manera aleatoria\n","episodes = 2\n","for episode in range(1, episodes+1):\n","    state = env.reset()  # Inicializamos el entorno\n","    done = False         # Flag de finalización de la partida (episodio)\n","    score = 0            # Contador de recompensas\n","\n","    # ! A Jugar ¡ (Hasta que termine la partida -> done == True)\n","    while not done:\n","        action = env.action_space.sample()              # Seleccionamos una acción Aleatoria\n","        n_state, reward, done, info = env.step(action)  # Realizamos la acción aleatoria y obtenemos:\n","                                                        # 1. Lista de estados, 2. Recompensa, 3. ¿Fin del juego?, 4. info\n","        score+=reward                                   # Sumamos la recompensa de la acción\n","    print('Episode:{} Score:{}'.format(episode, score))\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fk_H_mQnM8ND","executionInfo":{"status":"ok","timestamp":1724295752715,"user_tz":300,"elapsed":3043,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"4f52c072-5567-4b97-f765-0ac8ffbf45fa"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño de la imagen: 160x210 - Nº Canales: 3\n","\n","Nº de acciones: 6 - Acciones: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n","\n","Episode:1 Score:105.0\n","Episode:2 Score:105.0\n"]}]},{"cell_type":"code","source":["# 4.- Creamos un modelo de red neuronal convolucional\n","import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n","from tensorflow.keras.optimizers import Adam\n","\n","def build_cnn_model():\n","    model = Sequential()\n","    # The input_shape was corrected to (height, width, n_channels)\n","    model.add(Convolution2D(filters=32,\n","                            kernel_size=(8,8),\n","                            strides=(4,4),\n","                            activation='relu',\n","                            input_shape=(height, width, n_channels)))\n","    model.add(Convolution2D(filters=64,\n","                            kernel_size=(4,4),\n","                            strides=(2,2),\n","                            activation='relu'))\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu'))\n","    model.add(Dense(256, activation='relu'))\n","    model.add(Dense(n_actions, activation='linear'))\n","\n","    return model\n","\n","model = build_cnn_model()\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gcm50DXjOL_9","executionInfo":{"status":"ok","timestamp":1724295761404,"user_tz":300,"elapsed":526,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"0ac37c7a-9c8a-4061-af01-89a1ab251e49"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 51, 39, 32)        6176      \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 24, 18, 64)        32832     \n","                                                                 \n"," flatten (Flatten)           (None, 27648)             0         \n","                                                                 \n"," dense (Dense)               (None, 512)               14156288  \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 6)                 1542      \n","                                                                 \n","=================================================================\n","Total params: 14,328,166\n","Trainable params: 14,328,166\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from rl.agents import DQNAgent\n","from rl.memory import SequentialMemory\n","from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n","import gym\n","import atari_py\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n","from tensorflow.keras.optimizers import Adam\n","\n","# Definición de constantes\n","ENVIRONMENT_NAME = \"SpaceInvaders-v0\"\n","\n","# Instanciamos el Entorno\n","env = gym.make(ENVIRONMENT_NAME)\n","\n","# Obtenemos el tamaño de las imagenes del juego y los canales\n","height, width, n_channels = env.observation_space.shape\n","# Vemos las posibles acciones del juego:\n","n_actions = env.action_space.n\n","\n","# Number of frames to stack\n","window_length = 4\n","\n","def build_cnn_model():\n","    model = Sequential()\n","    # The input_shape now includes the window_length\n","    model.add(Convolution2D(filters=32,\n","                            kernel_size=(8,8),\n","                            strides=(4,4),\n","                            activation='relu',\n","                            input_shape=(window_length, height, width, n_channels)))\n","    model.add(Convolution2D(filters=64,\n","                            kernel_size=(4,4),\n","                            strides=(2,2),\n","                            activation='relu'))\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu'))\n","    model.add(Dense(256, activation='relu'))\n","    model.add(Dense(n_actions, activation='linear'))\n","\n","    return model\n","\n","model = build_cnn_model()\n","memory = SequentialMemory(limit=1000, window_length=window_length)\n","policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n","dqn = DQNAgent(model=model,\n","               memory=memory,\n","               policy=policy,\n","               enable_dueling_network=True,\n","               nb_actions=n_actions,\n","               nb_steps_warmup=100)\n","\n","# Compilamos el modelo\n","dqn.compile(Adam(lr=1e-4))\n","\n","# Entrenamos el modelo\n","dqn.fit(env, nb_steps=10000, visualize=False, verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lRtE1HBSOkEX","executionInfo":{"status":"ok","timestamp":1724330143552,"user_tz":300,"elapsed":405202,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"ecdda703-45ed-4510-af1b-e8581e923598"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Training for 10000 steps ...\n","Interval 1 (0 steps performed)\n","10000/10000 [==============================] - 34261s 3s/step - reward: 0.2070\n","done, took 34260.692 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7899f81146a0>"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# 6.- Explotamos el modelo. Nos ponemos a jugar con el agente ya entrenado.\n","env_test = gym.make(ENVIRONMENT_NAME, render_mode='human')\n","scores = dqn.test(env_test, nb_episodes=5, visualize=False)\n","print(np.mean(scores.history['episode_reward']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pr8fESL9DkW6","executionInfo":{"status":"ok","timestamp":1724332224326,"user_tz":300,"elapsed":385284,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"8f699470-3aea-4514-a638-5de7e8ee2da8"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing for 5 episodes ...\n","Episode 1: reward: 105.000, steps: 662\n","Episode 2: reward: 335.000, steps: 1049\n","Episode 3: reward: 105.000, steps: 762\n","Episode 4: reward: 120.000, steps: 655\n","Episode 5: reward: 145.000, steps: 905\n","162.0\n"]}]}]}