{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPradzcYvo/oeJMbZkRa2nL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BIgIBWQjdzIx","executionInfo":{"status":"ok","timestamp":1722861151634,"user_tz":300,"elapsed":6,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"4209c231-06ce-439d-c320-458197420c46"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'todo': 5, 'el': 6}\n","secuencias = [[1, 2], [1, 3, 4], [1, 3, 5, 6, 2]]\n"]}],"source":["import keras\n","import tensorflow as tf  # Import TensorFlow\n","frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo'\n","]\n","# Genera el diccionario de tokens\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10)\n","tokenizer.fit_on_texts(frases)\n","print(tokenizer.word_index)\n","\n","# Generación de secuencias tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =', secuencias)\n","\n","# Rellena las secuencias a una longuitud uniforme\n","relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('relleno =\\n', relleno)"]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZaK_ASxhyxu","executionInfo":{"status":"ok","timestamp":1722861224656,"user_tz":300,"elapsed":239,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"34ec64c2-1dd1-4076-9f94-5ee6bf36e4a5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["relleno =\n"," [[0 0 0 1 2]\n"," [0 0 1 3 4]\n"," [1 3 5 6 2]]\n"]}]},{"cell_type":"code","source":["frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen día, como estás hoy'\n","]\n","# Genera el diccionario de tokens\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10)\n","tokenizer.fit_on_texts(frases)\n","print(tokenizer.word_index)\n","\n","# Generación de secuencias tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =', secuencias)\n","\n","# Rellena las secuencias a una longuitud uniforme\n","relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('relleno =\\n', relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UO5BoZqfiD73","executionInfo":{"status":"ok","timestamp":1722861338431,"user_tz":300,"elapsed":256,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"000f3eee-94c7-451d-b727-8c21a866ace1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["{'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'todo': 5, 'el': 6, 'buen': 7, 'día': 8, 'como': 9, 'estás': 10, 'hoy': 11}\n","secuencias = [[1, 2], [1, 3, 4], [1, 3, 5, 6, 2], [7, 8, 9]]\n","relleno =\n"," [[0 0 0 1 2]\n"," [0 0 1 3 4]\n"," [1 3 5 6 2]\n"," [0 0 7 8 9]]\n"]}]},{"cell_type":"code","source":["frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen día, como estás hoy'\n","]\n","# Genera el diccionario de tokens\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(frases)\n","word_index = tokenizer.word_index\n","print('\\n word_index = ', word_index )\n","\n","# Generación de secuencias tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =', secuencias)\n","\n","# Rellena las secuencias a una longuitud uniforme\n","relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('relleno =\\n', relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKEpMBleijW5","executionInfo":{"status":"ok","timestamp":1722861603369,"user_tz":300,"elapsed":214,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"4710b598-5b97-4449-bbdb-5acbe36e4192"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," word_index =  {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'todo': 6, 'el': 7, 'buen': 8, 'día': 9, 'como': 10, 'estás': 11, 'hoy': 12}\n","secuencias = [[2, 3], [2, 4, 5], [2, 4, 6, 7, 3], [8, 9, 1, 1, 1]]\n","relleno =\n"," [[0 0 0 2 3]\n"," [0 0 2 4 5]\n"," [2 4 6 7 3]\n"," [8 9 1 1 1]]\n"]}]},{"cell_type":"code","source":["frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen día, como estás hoy'\n","]\n","# Genera el diccionario de tokens\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(frases)\n","word_index = tokenizer.word_index\n","print('\\n word_index = ', word_index )\n","\n","# Generación de secuencias tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =', secuencias)\n","\n","# Rellena las secuencias a una longuitud uniforme\n","relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias,\n","                                                        padding = 'post',\n","                                                        truncating = 'post')\n","print('relleno =\\n', relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722861725961,"user_tz":300,"elapsed":229,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"7e43c3e3-9027-46fa-f63d-d5f47b294312","id":"rWFwCgYfjlsS"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," word_index =  {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'todo': 6, 'el': 7, 'buen': 8, 'día': 9, 'como': 10, 'estás': 11, 'hoy': 12}\n","secuencias = [[2, 3], [2, 4, 5], [2, 4, 6, 7, 3], [8, 9, 1, 1, 1]]\n","relleno =\n"," [[2 3 0 0 0]\n"," [2 4 5 0 0]\n"," [2 4 6 7 3]\n"," [8 9 1 1 1]]\n"]}]}]}